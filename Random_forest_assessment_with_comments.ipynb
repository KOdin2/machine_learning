{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random forest assessment with comments",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzW1IseCFPpYEvAFwTcwC3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KOdin2/machine_learning/blob/main/Random_forest_assessment_with_comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyvgsejzKZ6"
      },
      "source": [
        "**Random forest**\n",
        "\n",
        "This code up is used to classify a dataset whose labels are a binary (either 1 and 0) a range of hyperparameters are tried. The best hyperparameters are used to calculate the Gini importance of each feature. The Gini importance allows us to remove features with a rank of 0, which means the model does not find it useful.\n",
        "\n",
        "**Note:** A uploaded dataset must have one column labeled \"Label\" with the row values as a 1 or 0 otherwise this code will not work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9GmNtOheAuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9283441b-f78e-4d06-de3c-9a542f9ce551"
      },
      "source": [
        "#Import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import io\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import csv\n",
        "\n",
        "\n",
        "#import functions from SKlearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#Import excel writer\n",
        "!pip install xlsxwriter\n",
        "import xlsxwriter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.7/dist-packages (1.4.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLbqdztR0RWI"
      },
      "source": [
        "The following code is used to upload as many files as you want to be classified"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "DOLCNaj8eLnw",
        "outputId": "f92cbfac-ae83-44f8-edc4-65df0910bba0"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ca839797-19ed-4443-bd55-0d71a5ad81ee\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ca839797-19ed-4443-bd55-0d71a5ad81ee\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Brass_BIM_dataset.csv to Brass_BIM_dataset (3).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXLnjg8sRn8S"
      },
      "source": [
        "The best measurements to use for classification are:\n",
        "\n",
        "* precision\n",
        "* recall\n",
        "* accuracy\n",
        "* f1\n",
        "* roc_auc\n",
        "\n",
        "Be careful with precision and recall as you can achieve 100% of one and have a really low other. F1 score is the harmoninc mean of precision and recall therefore should be used instread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXdLwan805nQ"
      },
      "source": [
        "**testing_combination_arrays**\n",
        "\n",
        "Used used to calcuate and create a list for the range used for hyperparamters\n",
        "\n",
        "n_esitmaters is between 1 and 20. max_features is half the max total of features\n",
        "\n",
        "**calculate_cross_validation_value**\n",
        "\n",
        "Is used to fond the largest fold value possible for CV "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcacs_-Qj9jy"
      },
      "source": [
        "def testing_combination_arrays(x_train_varible, max_varible):\n",
        "\n",
        "  \"\"\" Aquire the total columns of x_train and set the max features to half the value \"\"\"\n",
        "  x_train_columns = x_train_varible.shape[1]\n",
        "  x_train_columns = int(round((x_train_columns/2),0))\n",
        "  print(\"[INFO] Number of max features: \" + str(x_train_columns))\n",
        "\n",
        "  feature_combinations = [] \n",
        "  varible_combinations = []\n",
        "\n",
        "  \"\"\" Use a for loop to create a list for the max number of features \"\"\"\n",
        "  a = 1\n",
        "  for value in range(0,x_train_columns):\n",
        "    feature_combinations.append(a) \n",
        "    a+=1 \n",
        "\n",
        "  \"\"\" Use a for loop to create a list for the max number of esitmators \"\"\"\n",
        "  b = 1\n",
        "  for value in range(0, max_varible):\n",
        "    varible_combinations.append(b) \n",
        "    b+=1\n",
        "\n",
        "  \"\"\" Return the lists that are used for the gridsearch \"\"\"\n",
        "  return feature_combinations, varible_combinations\n",
        "\n",
        "\n",
        "def calculate_cross_validation_value(y_train):\n",
        "  \"\"\" aqurie the smallest number group of labels and return this for CV \"\"\"\n",
        "  arr = y_train.to_numpy()\n",
        "  bin_arr = np.bincount(arr)\n",
        "  return bin_arr.min()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgbNNePh1kyc"
      },
      "source": [
        "**grid_search_RF_function**\n",
        "\n",
        "This function is used to perfrom a grid search of defined hyperparameters\n",
        "\n",
        "Once complete this model will the models best parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzSWN1xihqF_"
      },
      "source": [
        "def grid_search_RF_function(x_train, y_train, Varible_list_1 , Varible_list_2, number_of_cv_folds):\n",
        "\n",
        "  \"\"\" Initalise the random forest model with the best hyperparameters and setup the MinMaxscaler() \"\"\"\n",
        "  model = RandomForestClassifier(random_state=0,bootstrap = False)  \n",
        "  min_max = MinMaxScaler()\n",
        "  \n",
        "  \"\"\" Setup the pipeline to perform scaling and model \"\"\"\n",
        "  pipe = Pipeline(steps = [\n",
        "                               (\"min_max\" , min_max ),\n",
        "                               \n",
        "                               (\"model\" , model )\n",
        "                               \n",
        "  ])\n",
        "\n",
        "  \"\"\" Set up the parameters for grid search with desired varibles for it to try \"\"\"\n",
        "  param_grid = [\n",
        "      {'model__n_estimators': Varible_list_1, 'model__max_features': Varible_list_2}\n",
        "  ]\n",
        "\n",
        "  \"\"\" Assign the scoring type for grid search \"\"\"\n",
        "  scoring_type = 'f1'\n",
        "  \n",
        "  \"\"\" Set up the gridsearch using defined varible \"\"\"\n",
        "  grid_search = GridSearchCV(estimator= pipe, param_grid= param_grid,  cv= number_of_cv_folds , scoring=scoring_type,  return_train_score=True, n_jobs=-1) #n_jobs=-1 will use all avaliable processors avaliable\n",
        "\n",
        "  \"\"\" Perfrom grid search with the x and y train data \"\"\"\n",
        "  grid_search.fit(X= x_train, y= y_train)\n",
        "\n",
        "  \"\"\" Aquire the best score and round to 4 decimal places and print the best parameters\"\"\"\n",
        "  best_accuracy = round(grid_search.best_score_,4)\n",
        "  print(\"[INFO] Best score: \" + str(best_accuracy) + \" with parameters: \"  + str(grid_search.best_params_) )\n",
        "\n",
        "  \"\"\" Create a list of the best results from the Gridsearch \"\"\"\n",
        "  best_model_details = ([ \"Random Forest\",\n",
        "                        scoring_type,\n",
        "                        best_accuracy,\n",
        "                        grid_search.best_params_['model__n_estimators'],\n",
        "                        grid_search.best_params_['model__max_features'],\n",
        "                        str(number_of_cv_folds.n_splits),\n",
        "                      \n",
        "  ])\n",
        "\n",
        "  \"\"\" Resturn the best model results and the parameters that will be used for feature reduction \"\"\" \n",
        "  return best_model_details, grid_search.best_params_"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvbDluQL2nAQ"
      },
      "source": [
        "**graph_important_features** \n",
        "\n",
        "Is used to graph the importance of the features\n",
        "\n",
        "As it is a function the values can be changed to make the graph clear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUlM27FscHZJ"
      },
      "source": [
        "def graph_important_features(model, x_train_df):\n",
        "\n",
        "  print(\"Feature importances of the input data obtained from (feature_importances_) for Random Forest\")\n",
        "\n",
        "  importance = np.abs(model.feature_importances_)\n",
        "  feature_names = np.array(x_train_df.columns)\n",
        "\n",
        "  f, ax = plt.subplots(figsize=(30,5))\n",
        "  plt.bar(height=importance, x=feature_names, )\n",
        "\n",
        "  plt.xticks(rotation='vertical', fontsize = 16)\n",
        "  plt.yticks(fontsize = 16)\n",
        "\n",
        "  plt.xlabel(\"Feature\", fontsize = 20)\n",
        "  plt.ylabel(\"Gini importance\", fontsize = 20)\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w86ZaW6N2ym1"
      },
      "source": [
        "**reduce_input**\n",
        "\n",
        "This code uses the best model paramters from the grid search to calculate the Gini importance of each feature\n",
        "\n",
        "Once the Gini importace is calucalte any features with an importance of 0 is removed\n",
        "\n",
        "The function then returns the reduced dataset\n",
        "\n",
        "If no features are removed the function returns true, ending the while loop which reduces the number of inputs\n",
        "\n",
        "**best_model_test**\n",
        "\n",
        "THis functions uses the best hyperparameters and the reduced dataset to perform cross validation.\n",
        "\n",
        "This function then returns the results and are used as the final result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2_pnMoZlrEM"
      },
      "source": [
        "def reduce_input(x_train, y_train, number_of_cv_folds, best_paramater):\n",
        "\n",
        "  \"\"\" Initalise the random forest model with the best hyperparameters and setup the MinMaxscaler() \"\"\"\n",
        "  model = RandomForestClassifier(random_state=0,bootstrap = False, n_estimators = best_paramater['model__n_estimators'],max_features = best_paramater['model__max_features'])  \n",
        "  min_max = MinMaxScaler()\n",
        "  \n",
        "  \"\"\" Setup the pipeline to perform scaling and model \"\"\"\n",
        "  pipe = Pipeline(steps = [\n",
        "                               (\"min_max\" , min_max ),\n",
        "                               \n",
        "                               (\"model\" , model )                               \n",
        "  ])\n",
        "\n",
        "\n",
        "  \"\"\" Fit the pipeline with the train data \"\"\"\n",
        "  pipe = pipe.fit(x_train, y_train)\n",
        "\n",
        "  \"\"\" Create a copy of the x train dataframe \"\"\"  \n",
        "  updated_x_train = x_train.copy()\n",
        "\n",
        "  \"\"\" Loop through all feature importance values and remove any featues with a value of 0 from the updated dataframe\"\"\"\n",
        "  bad_inputs = 0\n",
        "  for index, Gini_value in enumerate(model.feature_importances_):\n",
        "    \n",
        "    if(Gini_value == 0):\n",
        "      updated_x_train = updated_x_train.drop([x_train.columns[index]], axis='columns')\n",
        "      bad_inputs +=1\n",
        "\n",
        "  \"\"\" Check to see how many features are removed, if 0 then assign end_of_reduction to True to stop the input reduction method \"\"\"\n",
        "  if(bad_inputs==0):\n",
        "    print(\"[INFO] Inputs cannot be further reduced!\")\n",
        "    end_of_reduction = True\n",
        "  else: \n",
        "    print(\"[INFO] Number of removed inputs: \" + str(bad_inputs))\n",
        "    end_of_reduction = False\n",
        "  \n",
        "  \"\"\" Plot a bar chart of the importantance of each features \"\"\"\n",
        "  graph_important_features(model, x_train)\n",
        "\n",
        "  \"\"\" return status True if featrue reduction can no longer happen and the updated x_train dataset\"\"\"\n",
        "  return end_of_reduction, updated_x_train\n",
        "\n",
        "\n",
        "\n",
        "def best_model_test(x_train, y_train, number_of_cv_folds, best_paramater):\n",
        "\n",
        "  \"\"\" Initalise the random forest model with the best hyperparameters and setup the MinMaxscaler() \"\"\"\n",
        "  model = RandomForestClassifier(random_state=0,bootstrap = False, n_estimators = best_paramater['model__n_estimators'],max_features = best_paramater['model__max_features'])  \n",
        "  min_max = MinMaxScaler()\n",
        "  \n",
        "  \"\"\" Setup the pipeline to perform scaling and model \"\"\"\n",
        "  pipe = Pipeline(steps = [\n",
        "                               (\"min_max\" , min_max ),\n",
        "                               \n",
        "                               (\"model\" , model )\n",
        "                               \n",
        "  ])\n",
        "\n",
        "  \"\"\" Use cross_val_predict to perform cross validation and get the predicted labels \"\"\"\n",
        "  y_train_pred = cross_val_predict(pipe, x_train, y_train, cv=number_of_cv_folds)\n",
        "\n",
        "  \"\"\" Use the labels to produce a confusion matrix \"\"\"\n",
        "  conf = confusion_matrix(y_train, y_train_pred)\n",
        "  print(conf)\n",
        "\n",
        "  \"\"\" Calcualte the accuracies to .4dp which are then printed\"\"\"\n",
        "  f1        = round(f1_score(y_train, y_train_pred), 4)\n",
        "  precision = round(precision_score(y_train, y_train_pred), 4)\n",
        "  recall    = round(recall_score(y_train, y_train_pred), 4)\n",
        "\n",
        "  print(\"F1 score\\tPurity\\t\\tRecovery\")\n",
        "  print(str(f1) + \"\\t\\t\" + str(precision) + \"\\t\\t\" + str(recall) + \"\\n\\n\")\n",
        "    \n",
        "  \"\"\" Write the actual labels and predicted labels to a list. Work out whether the preidction was correct, if so write correct. This is for easy reference \"\"\"\n",
        "  predictions_vs_acutal = []\n",
        "  for index, row_data in enumerate(y_train):  \n",
        "      if y_train[index] == y_train_pred[index]:\n",
        "        correct_str = \"Correct\"\n",
        "      else:\n",
        "        correct_str = \"Incorrect\"\n",
        "      predictions_vs_acutal.insert(index, [y_train[index], y_train_pred[index], correct_str]   )\n",
        "\n",
        "\n",
        "  \"\"\" Create a list that has the accuracy results of the best model \"\"\"\n",
        "  model_check = ([str(f1), str(precision), str(recall), str(conf), str(len(x_train.columns)),  str(number_of_cv_folds.n_splits)])\n",
        "\n",
        "\n",
        "  return predictions_vs_acutal, model_check"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZr1_8EB4pIG"
      },
      "source": [
        "Headers used for the exported CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo0UkgDGy7iV"
      },
      "source": [
        "def write_results_to_excel (file_name, grid_search_list, model_check, best_features, predictions_vs_acutal):\n",
        "\n",
        "  \"\"\" Create headers used for the output file \"\"\"\n",
        "  header = [\"Algorithm\", \"Scoring type\", \"Score\", \"Number of esitmators\",  \"Max features\", \"Stratified KFold splits\"]\n",
        "  model_check_header = [\"F1 score\", \"Purity\", \"Recovery\", \"Confusion matrix\", \"Number of inputs\", \"Stratified KFold splits\"]\n",
        "  predicion_header = [\"Label\" , \"Prediction\", \"Result\"]\n",
        "\n",
        "  \"\"\"Create the workbook \"\"\"\n",
        "  workbook = xlsxwriter.Workbook(str(file_name) + '.xlsx')\n",
        "  worksheet = workbook.add_worksheet(\"Random Forest\")\n",
        "\n",
        "  \"\"\" Wrtie the Grid search results \"\"\"\n",
        "  worksheet.write(0, 0, \"Grid search results\")\n",
        "  for row_num, row_data in enumerate(header):\n",
        "    worksheet.write(1, row_num, row_data) \n",
        "  row = 2\n",
        "  for row_num, row_data in enumerate(grid_search_list):\n",
        "      for col_num, col_data in enumerate(row_data):\n",
        "          worksheet.write(row, col_num, str(col_data))\n",
        "      row+=1\n",
        "  row+=1\n",
        "\n",
        "  \"\"\" Write the best model results for final CV test with reduced dataset\"\"\"\n",
        "  worksheet.write(row, 0, \"Best model testing result\")\n",
        "  row+=1\n",
        "  for row_num, row_data in enumerate(model_check_header):\n",
        "    worksheet.write(row, row_num, row_data)\n",
        "  row +=1\n",
        "  for row_num, row_data in enumerate(model_check):\n",
        "    worksheet.write(row, row_num, row_data) \n",
        "  row +=2\n",
        "\n",
        "  \"\"\" Write all important features\"\"\"\n",
        "  worksheet.write(row, 0, \"Imporant Features\")\n",
        "  row +=1\n",
        "  for row_num, row_data in enumerate(best_features):\n",
        "      for col_num, col_data in enumerate(row_data):\n",
        "          worksheet.write(row, col_num, str(col_data))\n",
        "      row+=1\n",
        "\n",
        "  row+=1\n",
        "  \"\"\" Write the predicted values againest actual \"\"\"\n",
        "  worksheet.write(row, 0, \"Prediction results for the best model\")\n",
        "  row +=1 \n",
        "  for row_num, row_data in enumerate(predicion_header):\n",
        "    worksheet.write(row, row_num, row_data) \n",
        "  row+=1\n",
        "\n",
        "  for row_num, row_data in enumerate(predictions_vs_acutal):\n",
        "      for col_num, col_data in enumerate(row_data):\n",
        "          worksheet.write(row, col_num, str(col_data))\n",
        "      row+=1\n",
        "\n",
        "  \"\"\" Close the workboot and download\"\"\"\n",
        "  workbook.close()\n",
        "  files.download(str(file_name) + '.xlsx') \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "sugsesBVDuLK",
        "outputId": "832862a5-97bf-43c1-e3b5-d7677d62f637"
      },
      "source": [
        "print(\"[INFO] CODE START\")\n",
        "\n",
        "\"\"\" Loop through all uploaded files\"\"\"\n",
        "for file_name in uploaded:\n",
        "    grid_search_list = []  \n",
        "    best_features = []\n",
        "    reduction_mode = False\n",
        "\n",
        "    \"\"\" Convert the uploaded .csv file to a dataframe \"\"\"\n",
        "    print(\"File uploaded: \" +str(file_name))\n",
        "    loaded_file = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "\n",
        "    \"\"\" Create the x and y train datasets\"\"\"\n",
        "    x_train  = loaded_file.drop(['Label'], axis='columns')\n",
        "    y_train     = loaded_file.Label\n",
        "\n",
        "    \"\"\" Calculate the largest of number of fold for StratifiedKfold cross-vaildation\"\"\"\n",
        "    number_of_cv_folds = StratifiedKFold(calculate_cross_validation_value(y_train)) #This function can be changed to a int if a dataset is large as it will take long to run, e.g. 10\n",
        "    print(\"[INFO] Number of CV folds: \" + str(number_of_cv_folds.n_splits))\n",
        "\n",
        "    \"\"\" Calculate and create a list of the different varibles for the GridSearchCV, it takes the x train and the max value for max features\"\"\"\n",
        "    max_featutures_var, n_estitmators_var = testing_combination_arrays(x_train, 20) #This function can be removed and a manualy written list can be used instead\n",
        "    print(\"[INFO] N_estimators combinations: \" + str(n_estitmators_var))\n",
        "    print(\"[INFO] max_feature combinations: \" + str(max_featutures_var) + \"\\n\\n\")\n",
        "\n",
        "    \"\"\" Stay in a while loop while in inputs are reduced, if the inputs cannot be reduced any further break the loop \"\"\"\n",
        "    while(reduction_mode==False):\n",
        "\n",
        "      \"\"\" Perform Gird search for the best hyperparameters and appended results to a list\"\"\"\n",
        "      print(\"[INFO] Finding best model parameters\")\n",
        "      grid_search_result, best_paramater = grid_search_RF_function(x_train, y_train, n_estitmators_var, max_featutures_var, number_of_cv_folds)   \n",
        "      grid_search_list.append(grid_search_result)\n",
        "\n",
        "      \"\"\" Reduce the inputs of the x train dataset using the Gini importance and append the best features to a list\"\"\"\n",
        "      print(\"[INFO] Reducing input data based on best model\")\n",
        "      reduction_mode, x_train = reduce_input(x_train, y_train, number_of_cv_folds, best_paramater)\n",
        "      best_features.append(x_train.columns)\n",
        "\n",
        "    \"\"\" Using the best parameters and reduced dataset aquire the accuracy results and preductions \"\"\"\n",
        "    print(\"[INFO] Final cross-fold valudation test using the reduced input data and best parameters\")\n",
        "    predictions_vs_acutal, model_check = best_model_test(x_train, y_train, number_of_cv_folds, best_paramater,)\n",
        "    \n",
        "    \"\"\" Write all the results of the .csv file to an excel work book \"\"\"\n",
        "    write_results_to_excel(file_name, grid_search_list, model_check, best_features, predictions_vs_acutal)  \n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] CODE START\n",
            "File uploaded: Brass_BIM_dataset.csv\n",
            "[INFO] Number of CV folds: 17\n",
            "[INFO] Number of max features: 34\n",
            "[INFO] N_estimators combinations: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
            "[INFO] max_feature combinations: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
            "\n",
            "\n",
            "[INFO] Finding best model parameters\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-50a08c8b4426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;34m\"\"\" Perform Gird search for the best hyperparameters and appended results to a list\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Finding best model parameters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mgrid_search_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_paramater\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_RF_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estitmators_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_featutures_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_cv_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0mgrid_search_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-55dd4e0737c4>\u001b[0m in \u001b[0;36mgrid_search_RF_function\u001b[0;34m(x_train, y_train, Varible_list_1, Varible_list_2, number_of_cv_folds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;34m\"\"\" Perfrom grid search with the x and y train data \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;34m\"\"\" Aquire the best score and round to 4 decimal places and print the best parameters\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}